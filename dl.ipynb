{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset split - Train: 40000, Validation: 20000\n",
      "Created dataset split - Train: 50000, Validation: 10000\n",
      "Created dataset split - Train: 30000, Validation: 30000\n",
      "Created dataset split - Train: 55000, Validation: 5000\n",
      "Created dataset split - Train: 58000, Validation: 2000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Define dataset splits\n",
    "splits = [\n",
    "    (40000, 20000),\n",
    "    (50000, 10000),\n",
    "    (30000, 30000),\n",
    "    (55000, 5000),\n",
    "    (58000, 2000)\n",
    "]\n",
    "\n",
    "# Create different train-validation splits\n",
    "datasets_splits = []\n",
    "\n",
    "for train_size, val_size in splits:\n",
    "    train_set, val_set = random_split(mnist_train, [train_size, val_size])\n",
    "    datasets_splits.append((train_set, val_set))\n",
    "    print(f\"Created dataset split - Train: {train_size}, Validation: {val_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train: 40000, Validation: 20000 ===\n",
      "Batch Size: 10 --> Train Batches: 4000, Validation Batches: 2000\n",
      "Batch Size: 25 --> Train Batches: 1600, Validation Batches: 800\n",
      "Batch Size: 64 --> Train Batches: 625, Validation Batches: 313\n",
      "Batch Size: 128 --> Train Batches: 313, Validation Batches: 157\n",
      "Batch Size: 256 --> Train Batches: 157, Validation Batches: 79\n",
      "Batch Size: 512 --> Train Batches: 79, Validation Batches: 40\n",
      "\n",
      "=== Train: 50000, Validation: 10000 ===\n",
      "Batch Size: 10 --> Train Batches: 5000, Validation Batches: 1000\n",
      "Batch Size: 25 --> Train Batches: 2000, Validation Batches: 400\n",
      "Batch Size: 64 --> Train Batches: 782, Validation Batches: 157\n",
      "Batch Size: 128 --> Train Batches: 391, Validation Batches: 79\n",
      "Batch Size: 256 --> Train Batches: 196, Validation Batches: 40\n",
      "Batch Size: 512 --> Train Batches: 98, Validation Batches: 20\n",
      "\n",
      "=== Train: 30000, Validation: 30000 ===\n",
      "Batch Size: 10 --> Train Batches: 3000, Validation Batches: 3000\n",
      "Batch Size: 25 --> Train Batches: 1200, Validation Batches: 1200\n",
      "Batch Size: 64 --> Train Batches: 469, Validation Batches: 469\n",
      "Batch Size: 128 --> Train Batches: 235, Validation Batches: 235\n",
      "Batch Size: 256 --> Train Batches: 118, Validation Batches: 118\n",
      "Batch Size: 512 --> Train Batches: 59, Validation Batches: 59\n",
      "\n",
      "=== Train: 55000, Validation: 5000 ===\n",
      "Batch Size: 10 --> Train Batches: 5500, Validation Batches: 500\n",
      "Batch Size: 25 --> Train Batches: 2200, Validation Batches: 200\n",
      "Batch Size: 64 --> Train Batches: 860, Validation Batches: 79\n",
      "Batch Size: 128 --> Train Batches: 430, Validation Batches: 40\n",
      "Batch Size: 256 --> Train Batches: 215, Validation Batches: 20\n",
      "Batch Size: 512 --> Train Batches: 108, Validation Batches: 10\n",
      "\n",
      "=== Train: 58000, Validation: 2000 ===\n",
      "Batch Size: 10 --> Train Batches: 5800, Validation Batches: 200\n",
      "Batch Size: 25 --> Train Batches: 2320, Validation Batches: 80\n",
      "Batch Size: 64 --> Train Batches: 907, Validation Batches: 32\n",
      "Batch Size: 128 --> Train Batches: 454, Validation Batches: 16\n",
      "Batch Size: 256 --> Train Batches: 227, Validation Batches: 8\n",
      "Batch Size: 512 --> Train Batches: 114, Validation Batches: 4\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define batch sizes\n",
    "batch_sizes = [10, 25, 64, 128, 256, 512]\n",
    "\n",
    "# Store results\n",
    "dataloaders_info = []\n",
    "\n",
    "# Iterate over each dataset split\n",
    "for (train_set, val_set), (train_size, val_size) in zip(datasets_splits, splits):\n",
    "    print(f\"\\n=== Train: {train_size}, Validation: {val_size} ===\")\n",
    "    \n",
    "    # Iterate over different batch sizes\n",
    "    for batch_size in batch_sizes:\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        num_train_batches = len(train_loader)\n",
    "        num_val_batches = len(val_loader)\n",
    "        \n",
    "        dataloaders_info.append((train_size, val_size, batch_size, num_train_batches, num_val_batches))\n",
    "\n",
    "        print(f\"Batch Size: {batch_size} --> Train Batches: {num_train_batches}, Validation Batches: {num_val_batches}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
